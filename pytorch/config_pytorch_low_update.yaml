training:
  batch_size: 128
  learning_rate: 0.001
  loss: huber
  num_episodes: 10000
  train_steps: 1000000
  warmup_episode: 10
  save_freq: 1000

optimizer:
  name: adam
  lr_min: 0.0001
  lr_decay: 5000

rl:
  gamma: 0.99
  max_steps_per_episode: 100
  target_model_update_episodes: 200
  max_queue_length: 50000

epsilon:
  max_epsilon: 1
  min_epsilon: 0.1
  decay_epsilon: 400